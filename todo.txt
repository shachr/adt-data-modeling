1. Schema Management
    1.1. ADT composition forward transitive compatibility check
        1.1.1. consistent universal compatibility checks
        1.1.2. schema processing engine that uses a standard schema registry
    1.2. consider using reactive-streams:
        https://www.reactive-streams.org/
        https://github.com/reactive-streams/reactive-streams-jvm/tree/v1.0.4#specification

        OR camel routes ( comes with configuration ):
        https://camel.apache.org/manual/routes.html
    1.3. extract semantic context such as DDD domain concept, and relationships
        1.3.1 standardize json-schema
        1.3.2 standardize avro-schema
        1.3.3 standardize proto-schema
        1.3.3 enable external decoration for delta/parquet

2. Data Validation
    2.1 Json-schema for json payload validation
    2.2 Protobuf-validations? for proto validation
    2.3 Avro-validations? for proto validation
    2.4 Parquet use spark or other framework to validate - may leverage a library
    2.5 Delta has built in schema support

3. Data Parsing
    3.1 Parsing Json via Spark using ADT schema transformation to Spark-schema
    3.2 Parsing Avro using Avro sdk
    3.3 Parsing Proto using Proto sdk
    3.4 Parsing Parquet using Parquet sdk
    3.5 Parsing Delta using Delta sdk

4. Processing Pipeline
    using Databricks

5. Development Assets and Data Assets
    using Backstage

6. Identity and Authorization
    TBD